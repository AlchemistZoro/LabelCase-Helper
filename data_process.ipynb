{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_table(tree):\n",
    "    table= []\n",
    "    for i in tree:\n",
    "        a = i[\"value\"]\n",
    "        for j in i[\"children\"]:\n",
    "            b = j[\"value\"]\n",
    "            for z in j[\"children\"]:\n",
    "                c=z[\"value\"]\n",
    "                table.append([a,b,c])\n",
    "    return table\n",
    "\n",
    "def get_level3labels(tree):\n",
    "    level3labels = []\n",
    "    for t in tree:\n",
    "        level1 = t['value']\n",
    "        children1 = t['children']\n",
    "        for child1 in children1:\n",
    "            level2 = child1['value']\n",
    "            children2 = child1['children']\n",
    "            for child2 in children2:\n",
    "                level3 = child2['value']\n",
    "                level3labels.append('/'.join([level1, level2, level3]))\n",
    "    return level3labels\n",
    "\n",
    "def labeltree_dataset(textual_tree,code_tree,process_data_path):\n",
    "    textual_table = []\n",
    "    code_table = []\n",
    "\n",
    "\n",
    "    textual_table = tree_to_table(textual_tree)\n",
    "    code_table = tree_to_table(code_tree)\n",
    "    textual_frame = pd.DataFrame(textual_table,columns=[\"A\",\"B\",\"C\"])\n",
    "    code_frame = pd.DataFrame(code_table,columns=[\"A\",\"B\",\"C\"])\n",
    "\n",
    "    text_code_frame = pd.concat([code_frame[\"C\"],textual_frame[\"C\"]],axis=1)\n",
    "    text_code_frame.columns = [\"code_label\",\"textual_label\"]\n",
    "\n",
    "    code_dict = dict(zip(code_frame[\"C\"],textual_frame[\"C\"]))\n",
    "    # print(\"code_dict size:\",len(code_dict))\n",
    "    # print(\"code_frame size\",code_frame.shape[0])\n",
    "    # print(\"text_frame size\",textual_frame.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    return textual_frame,code_frame,text_code_frame\n",
    "\n",
    "\n",
    "def create_train_dataset(textual_frame,code_frame,data,process_data_path):\n",
    "    columns = ['id','label','content']\n",
    "    label_size=textual_frame.shape[0]\n",
    "    process_data = []\n",
    "    y=0\n",
    "    label_set = set()\n",
    "    label_c_set = set()\n",
    "    for i in tqdm(data):\n",
    "        content_list = []\n",
    "        for j in i[\"content\"]:\n",
    "            content_list.append([i[\"id\"],label_size,j])\n",
    "    \n",
    "        for j in i[\"evidence\"]:\n",
    "            idx = j[\"index\"]\n",
    "            value_list = j[\"value\"]\n",
    "            \n",
    "            pro_value_list = []\n",
    "            for z in value_list:\n",
    "                label_set.add(z)\n",
    "                code_label = z.split('/')[-1]\n",
    "                label_c_set.add(code_label)\n",
    "                num_label = code_frame[code_frame[\"C\"]==code_label].index.values[0]\n",
    "                pro_value_list.append(str(num_label))\n",
    "                \n",
    "            content_list[idx][1] = '#'.join(pro_value_list)\n",
    "        \n",
    "\n",
    "        if not process_data:\n",
    "            process_data = content_list\n",
    "        else:\n",
    "            process_data=process_data+content_list\n",
    "    # 234\n",
    "    # print(len(label_set))\n",
    "    # print(len(label_c_set))\n",
    "    dataset = pd.DataFrame(process_data,columns=columns)\n",
    "    # \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_vectorlabel_dataset(data,process_data_path):\n",
    "    vectorlabel_list = []    \n",
    "    case_label_num = []\n",
    "    label_num=np.zeros(234,'int64')\n",
    "    for i in data:\n",
    "        vectorlabel = np.zeros(235,'int64')\n",
    "        vectorlabel[0]=int(i[\"id\"])\n",
    "        for j in i[\"result\"]: \n",
    "            idx=int(j.split('/')[-1][1:])\n",
    "            vectorlabel[idx+1] = 1\n",
    "            label_num[idx]+=1\n",
    "        case_label_num.append(len(i[\"result\"]))    \n",
    "        vectorlabel_list.append(vectorlabel)\n",
    "\n",
    "    vec_frame = pd.DataFrame(np.array(vectorlabel_list))\n",
    "    # vec_frame.to_csv(process_data_path+'vec_frame.csv',index = False)\n",
    "    return vec_frame,case_label_num,label_num\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "process_data_path = './processeddata/'\n",
    "if not os.path.exists(process_data_path):\n",
    "    os.mkdir(process_data_path)\n",
    "\n",
    "code_tree=json.load(open('./rawdata/code_tree.json', encoding='utf-8'))\n",
    "textual_tree = json.load(open('./rawdata/textual_tree.json', encoding='utf-8'))\n",
    "code_list = get_level3labels(code_tree)\n",
    "textual_list = get_level3labels(textual_tree)\n",
    "# 216\n",
    "# print(len(code_list))\n",
    "# print(len(textual_list))\n",
    "raw_data=json.load(open('./rawdata/train.json', encoding='utf-8'))\n",
    "\n",
    "'''\n",
    "textual_frame: [A,B,C] 三级标签-中文\n",
    "code_frame: [A,B,C] 三级标签-代码\n",
    "text_code_frame: [C1,C2] 三级标签映射\n",
    "'''\n",
    "textual_frame,code_frame,text_code_frame=labeltree_dataset(textual_tree,code_tree,process_data_path)\n",
    "map_path='map/'\n",
    "if not os.path.exists(process_data_path+map_path):\n",
    "    os.mkdir(process_data_path+map_path)\n",
    "textual_frame.to_csv(process_data_path+map_path+'textual_frame.csv',index=False)\n",
    "code_frame.to_csv(process_data_path+map_path+'code_frame.csv',index=False)\n",
    "text_code_frame.to_csv(process_data_path+map_path+'text_code_frame.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2496/2496 [00:17<00:00, 141.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "train_frame:[id,label,text]\n",
    "'''\n",
    "data=create_train_dataset(textual_frame,code_frame,raw_data,process_data_path)\n",
    "data.to_csv(process_data_path+'train.csv',index = False)\n",
    "'''\n",
    "vec_frame:[id,[label_vec]]\n",
    "'''\n",
    "case_data,case_label_num,label_num=create_vectorlabel_dataset(raw_data,process_data_path)\n",
    "case_data.to_csv(process_data_path+'case.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38040it [00:23, 1719.22it/s]"
     ]
    }
   ],
   "source": [
    "# 删除没有正类的样本集\n",
    "section_set = set()\n",
    "delete_sections = ['当事人信息','再审被申请人辩称','被上诉人答辩','审判人员','裁判结果','开始','']\n",
    "pre_id = data.loc[0][0]\n",
    "pre_key = '开始'\n",
    "now_key = '开始'\n",
    "section_labels=[]\n",
    "for i,row in tqdm(data.iterrows()):\n",
    "    now_id=row[\"id\"]\n",
    "    if pre_id !=now_id:\n",
    "        now_key = \"开始\"\n",
    "    if row['content'] != \"\" and row[\"content\"][0] =='【':\n",
    "        now_key = row['content'][1:-1]\n",
    "        section_set.add(row['content'])\n",
    "    row['section_label'] = now_key\n",
    "    section_labels.append(now_key)\n",
    "    pre_key=now_key \n",
    "    pre_id=now_id \n",
    "data['section_label']=section_labels\n",
    "data=data[~data[\"section_label\"].isin(delete_sections)]\n",
    "print('删除全负section样本后的个数：',data.shape[0])\n",
    "data=data[~data[\"content\"].isin(section_set)]\n",
    "print('删除头样本后的个数：',data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210930it [00:14, 14845.87it/s]\n"
     ]
    }
   ],
   "source": [
    "onehot_labels = []\n",
    "class_num = 234\n",
    "for i,row in tqdm(data.iterrows()):\n",
    "    labels = np.zeros(class_num)\n",
    "    if row[\"label\"] == class_num:\n",
    "        onehot_labels.append(labels)\n",
    "        continue    \n",
    "    for idx in row[\"label\"].split(\"#\"):\n",
    "        labels[int(idx)] = 1\n",
    "    onehot_labels.append(labels)\n",
    "onehot_labels = np.array(onehot_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210930, 238)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "onehot_labels=pd.DataFrame(onehot_labels,columns=[i for i in range(class_num)])\n",
    "data=data.reset_index(drop=True)\n",
    "data_onehot=pd.concat([data,onehot_labels],axis=1)\n",
    "data_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2496/2496 [03:01<00:00, 13.77it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "content_size = 500 # content_size = [100]\n",
    "data_windows = []\n",
    "content_label = np.zeros(234)\n",
    "for i,group in tqdm(data_onehot.groupby('id')):\n",
    "    id = group.iloc[0]['id']\n",
    "    content_windows = ''\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    while len(content_windows)+len(group.iloc[end_index]['content'])<content_size:\n",
    "        content_label=content_label+group.iloc[end_index].values[4:]\n",
    "        content_windows+=group.iloc[end_index]['content']\n",
    "        end_index+=1\n",
    "    # for idx,row in group.iterrows():\n",
    "    current_label = content_label\n",
    "    current_label[current_label>1] =1\n",
    "    current_label = list(current_label)\n",
    "    current_label.append(id)\n",
    "    current_label.append(content_windows)\n",
    "    data_windows.append(current_label)\n",
    "    while end_index<group.shape[0]:\n",
    "        \n",
    "        content_label=content_label+group.iloc[end_index].values[4:]\n",
    "        content_windows+=group.iloc[end_index]['content']\n",
    "        end_index+=1\n",
    "        while len(content_windows)>content_size:\n",
    "            content_label=content_label-group.iloc[start_index].values[4:]\n",
    "            start_size = len(group.iloc[start_index]['content'])\n",
    "            content_windows= content_windows[start_size:]\n",
    "            start_index+=1\n",
    "        current_label = content_label\n",
    "        current_label[current_label>1] =1\n",
    "        current_label = list(current_label)\n",
    "        current_label.append(id)\n",
    "        current_label.append(content_windows)\n",
    "        data_windows.append(current_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dw = [str(i) for i in range(class_num) ]\n",
    "columns_dw.append('id')\n",
    "columns_dw.append('content')\n",
    "data_dw_frame=pd.DataFrame( data_windows,columns=columns_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label shape: (2496, 235)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "r=[0.8,0.9,1]，r=1：全量训练模型 \n",
    "'''\n",
    "train_rate=1\n",
    "\n",
    "train_data_path = process_data_path+\"tr-\"+str(train_rate)+'/'\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.mkdir(train_data_path)\n",
    "\n",
    "train_label=case_data.sample(frac=train_rate,replace=False) #抽取20%的数据\n",
    "train_label.to_csv(train_data_path+'train_label.csv',index = False)\n",
    "train_data = data_dw_frame[data_dw_frame[\"id\"].isin(train_label[0])]\n",
    "train_data.to_csv(train_data_path+'train_data.csv',index = False)\n",
    "\n",
    "print('train label shape:',train_label.shape)\n",
    "\n",
    "if not train_rate==1:\n",
    "    valid_label=case_data[~case_data.index.isin(train_label.index)]\n",
    "    valid_label.to_csv(train_data_path+'valid_label.csv',index = False)\n",
    "    valid_data = data_dw_frame[data_dw_frame[\"id\"].isin(valid_label[0])]\n",
    "    valid_data.to_csv(train_data_path+'valid_data.csv',index = False)\n",
    "    print('valid label shape:',valid_label.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b09f0dae079356b11e2992c8ce1698bd60fda55aea4c87f004ec164747e9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
