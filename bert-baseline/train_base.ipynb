{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from model import CaseClassification\n",
    "from transformers import BertTokenizer, AdamW\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 16356\n",
      "正样本数: 1316\n",
      "负样本数: 15040\n",
      "正负样本比例: 0.0875\n",
      "(1447, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data_path = '../preprocessdata/'\n",
    "data = pd.read_csv(process_data_path+'train.csv')\n",
    "\n",
    "# drop nan text \n",
    "data=data[data[\"content\"].notnull()]\n",
    "\n",
    "vec_frame = pd.read_csv(process_data_path+'vec_frame.csv')\n",
    "if debug:\n",
    "    vec_frame = vec_frame.head(200)\n",
    "train_rate = 0.80\n",
    "case_size = vec_frame.shape[0]\n",
    "\n",
    "\n",
    "train_index_list = vec_frame.iloc[0:int(train_rate*case_size),0]\n",
    "valid_index_list = vec_frame.iloc[int(train_rate*case_size):,0]\n",
    "\n",
    "\n",
    "train_label = vec_frame[vec_frame[\"0\"].isin(train_index_list)]\n",
    "valid_label = vec_frame[vec_frame[\"0\"].isin(valid_index_list)]\n",
    "train_data = data[data[\"id\"].isin(train_index_list)]\n",
    "valid_data = data[data[\"id\"].isin(valid_index_list)]\n",
    "\n",
    "num = train_data.shape[0]\n",
    "p_num = train_data[train_data[\"label\"]!= '234'].shape[0]\n",
    "n_num = train_data[train_data[\"label\"]== '234'].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 10\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"label\"]=='234'].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"label\"]!='234']\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print(train_data_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,index_list,class_num):\n",
    "        self.data = data\n",
    "        self.index_list = index_list\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,2]\n",
    "        id = int(self.data.iloc[idx,0])\n",
    "        label_list = self.data.iloc[idx,1]\n",
    "        label = torch.zeros(self.class_num)\n",
    "        for i in label_list.split(\"#\"):\n",
    "            label[int(i)] = 1\n",
    "        \n",
    "        return id,fact, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 235\n",
    "train_dataset = CaseData(train_data_now,train_index_list,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_index_list,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=32,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check dataset\n",
    "# def check_dataset(data_set):\n",
    "#     for i in range(train_dataset.__len__()):\n",
    "#         a,b,c=train_dataset.__getitem__(i)  \n",
    "#         if not (isinstance (a,int) and isinstance(b,str) ):\n",
    "#             print(type(a),type(b),type(c),b)\n",
    "# check_dataset(valid_dataset)\n",
    "# print(valid_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train, step: 50, train_loss: 0.576\n",
      "Epoch 1 Sen Valid   acc: 0.9914, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 1 Case Valid   acc: 0.9590, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 2 Train, step: 50, train_loss: 0.329\n",
      "Epoch 2 Sen Valid   acc: 0.9914, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 2 Case Valid   acc: 0.9606, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 3 Train, step: 50, train_loss: 0.206\n",
      "Epoch 3 Sen Valid   acc: 0.9914, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 3 Case Valid   acc: 0.9606, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 4 Train, step: 50, train_loss: 0.141\n",
      "Epoch 4 Sen Valid   acc: 0.9914, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 4 Case Valid   acc: 0.9606, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 5 Train, step: 50, train_loss: 0.105\n",
      "Epoch 5 Sen Valid   acc: 0.9914, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 5 Case Valid   acc: 0.9606, pre: 0.0000, rec: 0.0000, f1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+1)\n",
    "    r = TP / (TP + FN+1)\n",
    "    F1 = 2 * r * p / (r + p+1)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN+1)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "def get_predict_label(logits,threshold):\n",
    "    for i in range(len(logits)):\n",
    "        for j in range(len(logits[0])):\n",
    "            if logits[i][j]>threshold: logits[i][j] = 1\n",
    "            else: logits[i][j] = 0\n",
    "    return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        id,fact, label= data\n",
    "\n",
    "        # tokenize the data text\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        \n",
    "        # forward and backward propagations\n",
    "        loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % print_diff == print_diff -1 :\n",
    "            print('Epoch %d Train, step: %2d, train_loss: %.3f' % (epoch + 1, i + 1, running_loss / print_diff))\n",
    "            # wandb.log({'train_loss': running_loss / 50})\n",
    "            running_loss = 0.0\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "\n",
    "    \n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        id,fact, c_label= data\n",
    "\n",
    "        # tokenize the data text\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # forward and backward propagations\n",
    "        loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "\n",
    "        threshold = 0.5\n",
    "        logits[logits>threshold] = 1\n",
    "        logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "        logits=logits.cpu().numpy()\n",
    "        # for i in range(logits.shape[0]):\n",
    "        #     c_predict=np.zeros(class_num)\n",
    "        #     c_predict[np.argmax(logits[i])]=1\n",
    "        #     logits[i]=c_predict\n",
    "        \n",
    "        # 单句标签f1\n",
    "        c_label = c_label.cpu().numpy()\n",
    "        for i in range(len(id)):\n",
    "            idx = int(id[i])\n",
    "            row_idx = valid_index_dict[idx]\n",
    "            predict[row_idx] += logits[i]\n",
    "        \n",
    "\n",
    "        for i in range(len(logits)):\n",
    "            acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "            # print(logits[0],label[1])       \n",
    "            total_precision+= p\n",
    "            total_recall+= r\n",
    "            total_f1+= F1\n",
    "            total_acc+= acc\n",
    "\n",
    "    # 案件标签f1\n",
    "    predict[predict>1] = 1\n",
    "    for i in range(predict.shape[0]):\n",
    "\n",
    "        acc,p,r,F1 = cal_metrics(predict[i][0:-1],label[i]) \n",
    "        case_precision+= p\n",
    "        case_recall+= r\n",
    "        case_f1+= F1\n",
    "        case_acc+= acc\n",
    "\n",
    "        \n",
    "    print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "    print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size,))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b09f0dae079356b11e2992c8ce1698bd60fda55aea4c87f004ec164747e9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
