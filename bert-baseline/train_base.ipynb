{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from model import CaseClassification\n",
    "from transformers import BertTokenizer, AdamW\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "process_data_path = '../preprocessdata/'\n",
    "data = pd.read_csv(process_data_path+'train.csv')\n",
    "\n",
    "# 大约有100多个content是空值，后面会留下错误\n",
    "data=data[data[\"content\"].notnull()]\n",
    "\n",
    "vec_frame = pd.read_csv(process_data_path+'vec_frame.csv')\n",
    "debug = True\n",
    "if debug:\n",
    "    vec_frame = vec_frame.head(20)\n",
    "train_rate = 0.80\n",
    "case_size = vec_frame.shape[0]\n",
    "# print(data.shape)\n",
    "# data.dropna(how='any',axis=0)\n",
    "# print(data.shape)\n",
    "\n",
    "train_index_list = vec_frame.iloc[0:int(train_rate*case_size),0]\n",
    "valid_index_list = vec_frame.iloc[int(train_rate*case_size):,0]\n",
    "\n",
    "\n",
    "train_label = vec_frame[vec_frame[\"0\"].isin(train_index_list)]\n",
    "valid_label = vec_frame[vec_frame[\"0\"].isin(valid_index_list)]\n",
    "train_data = data[data[\"id\"].isin(train_index_list)]\n",
    "valid_data = data[data[\"id\"].isin(valid_index_list)]\n",
    "\n",
    "num = train_data.shape[0]\n",
    "p_num = train_data[train_data[\"label\"]!= '234'].shape[0]\n",
    "n_num = train_data[train_data[\"label\"]== '234'].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 0.5\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"label\"]=='234'].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"label\"]!='234']\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print(train_data_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from model import CaseClassification\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,index_list,class_num):\n",
    "        self.data = data\n",
    "        self.index_list = index_list\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,2]\n",
    "        id = int(self.data.iloc[idx,0])\n",
    "        label_list = self.data.iloc[idx,1]\n",
    "        label = torch.zeros(self.class_num)\n",
    "        for i in label_list.split(\"#\"):\n",
    "            label[int(i)] = 1\n",
    "        \n",
    "        return id,fact, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 235\n",
    "train_dataset = CaseData(train_data_now,train_index_list,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_index_list,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data_now[\"id\"].unique())\n",
    "# print(train_index_list)\n",
    "a,b,c=train_dataset.__getitem__(1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 空文本会带来问题,用这个方法筛出来了\n",
    "def check_dataset(data_set):\n",
    "    for i in range(train_dataset.__len__()):\n",
    "        a,b,c=train_dataset.__getitem__(i)  \n",
    "        if not (isinstance (a,int) and isinstance(b,str) ):\n",
    "            print(type(a),type(b),type(c),b)\n",
    "check_dataset(valid_dataset)\n",
    "print(valid_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 235\n",
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train, step: 50, train_loss: 0.024\n",
      "Epoch 1 Train, step: 100, train_loss: 0.025\n",
      "Epoch 1 Train, step: 150, train_loss: 0.023\n",
      "Epoch 1 Train, step: 200, train_loss: 0.024\n",
      "Epoch 1 Train, step: 250, train_loss: 0.024\n",
      "Epoch 1 Train, step: 300, train_loss: 0.020\n",
      "Epoch 1 Train, step: 350, train_loss: 0.021\n",
      "Epoch 1 Train, step: 400, train_loss: 0.019\n",
      "Epoch 1 Train, step: 450, train_loss: 0.020\n",
      "Epoch 0 Sen Valid   acc: 0.9946, pre: 0.4429, rec: 0.4429, f1: 0.2215\n",
      "Epoch 0 Case Valid   acc: 0.9574, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 2 Train, step: 50, train_loss: 0.021\n",
      "Epoch 2 Train, step: 100, train_loss: 0.018\n",
      "Epoch 2 Train, step: 150, train_loss: 0.020\n",
      "Epoch 2 Train, step: 200, train_loss: 0.020\n",
      "Epoch 2 Train, step: 250, train_loss: 0.017\n",
      "Epoch 2 Train, step: 300, train_loss: 0.018\n",
      "Epoch 2 Train, step: 350, train_loss: 0.018\n",
      "Epoch 2 Train, step: 400, train_loss: 0.021\n",
      "Epoch 2 Train, step: 450, train_loss: 0.017\n",
      "Epoch 1 Sen Valid   acc: 0.9946, pre: 0.4429, rec: 0.4429, f1: 0.2215\n",
      "Epoch 1 Case Valid   acc: 0.9574, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 3 Train, step: 50, train_loss: 0.018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27400/3942118890.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[0mvalid_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27400/3942118890.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_diff\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprint_diff\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+1)\n",
    "    r = TP / (TP + FN+1)\n",
    "    F1 = 2 * r * p / (r + p+1)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN+1)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "def get_predict_label(logits,threshold):\n",
    "    for i in range(len(logits)):\n",
    "        for j in range(len(logits[0])):\n",
    "            if logits[i][j]>threshold: logits[i][j] = 1\n",
    "            else: logits[i][j] = 0\n",
    "    return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        id,fact, label= data\n",
    "\n",
    "        # tokenize the data text\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        \n",
    "        # forward and backward propagations\n",
    "        loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % print_diff == print_diff -1 :\n",
    "            print('Epoch %d Train, step: %2d, train_loss: %.3f' % (epoch + 1, i + 1, running_loss / print_diff))\n",
    "            # wandb.log({'train_loss': running_loss / 50})\n",
    "            running_loss = 0.0\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "\n",
    "    \n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        id,fact, c_label= data\n",
    "\n",
    "        # tokenize the data text\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # forward and backward propagations\n",
    "        loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "\n",
    "        # threshold = 0.5\n",
    "\n",
    "        # logits[logits>threshold] = 1\n",
    "        # logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "        logits=logits.cpu().numpy()\n",
    "        for i in range(logits.shape[0]):\n",
    "            c_predict=np.zeros(class_num)\n",
    "            c_predict[np.argmax(logits[i])]=1\n",
    "            logits[i]=c_predict\n",
    "            \n",
    "        c_label = c_label.cpu().numpy()\n",
    "        for i in range(len(id)):\n",
    "            idx = int(id[i])\n",
    "            row_idx = valid_index_dict[idx]\n",
    "            predict[row_idx] += logits[i]\n",
    "        \n",
    "\n",
    "        for i in range(len(logits)):\n",
    "            acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "            # print(logits[0],label[1])       \n",
    "            total_precision+= p\n",
    "            total_recall+= r\n",
    "            total_f1+= F1\n",
    "            total_acc+= acc\n",
    "\n",
    "\n",
    "    predict[predict>1] = 1\n",
    "\n",
    "    for i in range(predict.shape[0]):\n",
    "\n",
    "        acc,p,r,F1 = cal_metrics(predict[i][0:-1],label[i]) \n",
    "        case_precision+= p\n",
    "        case_recall+= r\n",
    "        case_f1+= F1\n",
    "        case_acc+= acc\n",
    "\n",
    "        \n",
    "    print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "    print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size,))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b09f0dae079356b11e2992c8ce1698bd60fda55aea4c87f004ec164747e9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
