{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接用二分类的loss去做，会涉及到阈值的问题不是很划算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, AdamW,AutoModel, AutoTokenizer\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug :\n",
    "    epochs = 2\n",
    "    train_batch = 16\n",
    "    valid_batch = 4\n",
    "else :\n",
    "    epochs = 20\n",
    "    train_batch = 16\n",
    "    valid_batch = 64\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# model_path = \"thunlp/Lawformer\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 231906\n",
      "正样本数: 18790\n",
      "负样本数: 213116\n",
      "正负样本比例: 0.08816794609508437\n",
      "训练样本数： (22548, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data_path = '../preprocessdata/'\n",
    "data = pd.read_csv(process_data_path+'train.csv')\n",
    "\n",
    "# drop nan text \n",
    "data=data[data[\"content\"].notnull()]\n",
    "\n",
    "vec_frame = pd.read_csv(process_data_path+'vec_frame.csv')\n",
    "if debug:\n",
    "    vec_frame = vec_frame.head(200)\n",
    "train_rate = 0.85\n",
    "case_size = vec_frame.shape[0]\n",
    "\n",
    "\n",
    "train_index_list = vec_frame.iloc[0:int(train_rate*case_size),0]\n",
    "valid_index_list = vec_frame.iloc[int(train_rate*case_size):,0]\n",
    "\n",
    "\n",
    "train_label = vec_frame[vec_frame[\"0\"].isin(train_index_list)]\n",
    "valid_label = vec_frame[vec_frame[\"0\"].isin(valid_index_list)]\n",
    "train_data = data[data[\"id\"].isin(train_index_list)]\n",
    "valid_data = data[data[\"id\"].isin(valid_index_list)]\n",
    "\n",
    "num = train_data.shape[0]\n",
    "p_num = train_data[train_data[\"label\"]!= '234'].shape[0]\n",
    "n_num = train_data[train_data[\"label\"]== '234'].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 5\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"label\"]=='234'].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"label\"]!='234']\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print('训练样本数：',train_data_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = []\n",
    "# for i in train_data_now[\"content\"]:\n",
    "#     c.append(len(i))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,index_list,class_num):\n",
    "        self.data = data\n",
    "        self.index_list = index_list\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,2]\n",
    "        id = int(self.data.iloc[idx,0])\n",
    "        label_list = self.data.iloc[idx,1]\n",
    "        label = torch.zeros(self.class_num)\n",
    "        for i in label_list.split(\"#\"):\n",
    "            if int(i) != 234:\n",
    "                label[int(i)] = 1\n",
    "        return id,fact, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 234\n",
    "train_dataset = CaseData(train_data_now,train_index_list,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_index_list,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multilabel_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    https://kexue.fm/archives/7359\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred  # -1 -> pos classes, 1 -> neg classes\n",
    "    y_pred_neg = y_pred - y_true * 1e12  # mask the pred outputs of pos classes\n",
    "    y_pred_pos = (y_pred - (1 - y_true) * 1e12)  # mask the pred outputs of neg classes\n",
    "    zeros = torch.zeros_like(y_pred[..., :1])\n",
    "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "    return (neg_loss + pos_loss).mean()\n",
    "    \n",
    "import torch.nn as nn\n",
    "class CaseClassification(nn.Module):\n",
    "    def __init__(self, class_num,model_path):\n",
    "        super(CaseClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        self.linear = nn.Linear(in_features=768, out_features=class_num)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, label=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_output = outputs['pooler_output']\n",
    "\n",
    "        logits = self.linear(pooler_output)\n",
    "        logits = torch.sigmoid(logits)\n",
    "        if label is not None:\n",
    "            loss_fn = nn.BCELoss()\n",
    "            # loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits, label)\n",
    "            # loss = multilabel_cross_entropy(logits, label)\n",
    "            return loss, logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = valid_label.values[:,1:]\n",
    "# valid_size = valid_data.shape[0]\n",
    "# valid_case_size = len(valid_label)\n",
    "# valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "# print(label.shape)\n",
    "# print(valid_label.iloc[:,0].values)\n",
    "# print(valid_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:12<00:00, 10.61it/s, loss=0.0151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.07083317419408378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.14it/s, loss=0.00938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.009975234507110011\n",
      "Epoch 1 Sen Valid   acc: 0.9995, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 1 Case Valid   acc: 0.9620, pre: 0.0000, rec: 0.0000, f1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.0205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.025601066205095738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.007652396985387824\n",
      "Epoch 2 Sen Valid   acc: 0.9995, pre: 0.0000, rec: 0.0000, f1: 0.0000\n",
      "Epoch 2 Case Valid   acc: 0.9620, pre: 0.0000, rec: 0.0000, f1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.023597791316053756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.11it/s, loss=0.00484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.005630915942903941\n",
      "Epoch 3 Sen Valid   acc: 0.9995, pre: 0.0069, rec: 0.0064, f1: 0.0065\n",
      "Epoch 3 Case Valid   acc: 0.9637, pre: 0.4848, rec: 0.0700, f1: 0.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.49it/s, loss=0.0264] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.018539403381935778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.0034] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.004309772755329427\n",
      "Epoch 4 Sen Valid   acc: 0.9994, pre: 0.0260, rec: 0.0220, f1: 0.0232\n",
      "Epoch 4 Case Valid   acc: 0.9667, pre: 0.6215, rec: 0.2548, f1: 0.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.49it/s, loss=0.0156] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.014688811990505098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.12it/s, loss=0.00285] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003800598811657445\n",
      "Epoch 5 Sen Valid   acc: 0.9992, pre: 0.0393, rec: 0.0341, f1: 0.0357\n",
      "Epoch 5 Case Valid   acc: 0.9666, pre: 0.5754, rec: 0.4183, f1: 0.4637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.00967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.012278758718612346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00258] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003717352029573368\n",
      "Epoch 6 Sen Valid   acc: 0.9991, pre: 0.0477, rec: 0.0426, f1: 0.0439\n",
      "Epoch 6 Case Valid   acc: 0.9643, pre: 0.5202, rec: 0.5417, f1: 0.5109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.46it/s, loss=0.00924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.010542735855874494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.14it/s, loss=0.00253] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.0034568727530058557\n",
      "Epoch 7 Sen Valid   acc: 0.9991, pre: 0.0485, rec: 0.0443, f1: 0.0451\n",
      "Epoch 7 Case Valid   acc: 0.9629, pre: 0.5068, rec: 0.5769, f1: 0.5194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.00544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.00918781871772008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.11it/s, loss=0.00185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003360936126496182\n",
      "Epoch 8 Sen Valid   acc: 0.9990, pre: 0.0519, rec: 0.0472, f1: 0.0482\n",
      "Epoch 8 Case Valid   acc: 0.9598, pre: 0.4745, rec: 0.6240, f1: 0.5199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.45it/s, loss=0.0108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.008063029583563355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003288186443608392\n",
      "Epoch 9 Sen Valid   acc: 0.9990, pre: 0.0525, rec: 0.0489, f1: 0.0494\n",
      "Epoch 9 Case Valid   acc: 0.9595, pre: 0.4785, rec: 0.6545, f1: 0.5324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.51it/s, loss=0.00236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.007121726717297912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.0033565305025099333\n",
      "Epoch 10 Sen Valid   acc: 0.9990, pre: 0.0531, rec: 0.0503, f1: 0.0502\n",
      "Epoch 10 Case Valid   acc: 0.9557, pre: 0.4480, rec: 0.6804, f1: 0.5213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.0102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.006311110249499242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.15it/s, loss=0.00182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.0037429492732263744\n",
      "Epoch 11 Sen Valid   acc: 0.9988, pre: 0.0551, rec: 0.0520, f1: 0.0520\n",
      "Epoch 11 Case Valid   acc: 0.9533, pre: 0.4306, rec: 0.6973, f1: 0.5134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.0119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.005596670695763834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.14it/s, loss=0.00174] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003625530992542545\n",
      "Epoch 12 Sen Valid   acc: 0.9989, pre: 0.0547, rec: 0.0526, f1: 0.0521\n",
      "Epoch 12 Case Valid   acc: 0.9539, pre: 0.4371, rec: 0.7241, f1: 0.5254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.50it/s, loss=0.000302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.005003700454340396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.11it/s, loss=0.00144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.0038577080340384856\n",
      "Epoch 13 Sen Valid   acc: 0.9988, pre: 0.0551, rec: 0.0539, f1: 0.0529\n",
      "Epoch 13 Case Valid   acc: 0.9495, pre: 0.4078, rec: 0.7376, f1: 0.5087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.47it/s, loss=0.000207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0044469127464434295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.12it/s, loss=0.00152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003596682231992254\n",
      "Epoch 14 Sen Valid   acc: 0.9989, pre: 0.0551, rec: 0.0525, f1: 0.0523\n",
      "Epoch 14 Case Valid   acc: 0.9525, pre: 0.4247, rec: 0.7155, f1: 0.5150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.46it/s, loss=0.00119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.003941987186392888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00251] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.003814538295297663\n",
      "Epoch 15 Sen Valid   acc: 0.9988, pre: 0.0554, rec: 0.0532, f1: 0.0527\n",
      "Epoch 15 Case Valid   acc: 0.9498, pre: 0.4086, rec: 0.7233, f1: 0.5047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.48it/s, loss=0.00102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0035547559653616887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.0021]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.004148813008713066\n",
      "Epoch 16 Sen Valid   acc: 0.9988, pre: 0.0567, rec: 0.0550, f1: 0.0542\n",
      "Epoch 16 Case Valid   acc: 0.9486, pre: 0.4021, rec: 0.7421, f1: 0.5041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.51it/s, loss=0.000986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.003183551672379592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.11it/s, loss=0.0027]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.0045114184821376776\n",
      "Epoch 17 Sen Valid   acc: 0.9986, pre: 0.0560, rec: 0.0552, f1: 0.0539\n",
      "Epoch 17 Case Valid   acc: 0.9437, pre: 0.3811, rec: 0.7665, f1: 0.4923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.49it/s, loss=0.00907] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002879678557960462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00165] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.0042657470703744834\n",
      "Epoch 18 Sen Valid   acc: 0.9988, pre: 0.0565, rec: 0.0555, f1: 0.0544\n",
      "Epoch 18 Case Valid   acc: 0.9469, pre: 0.3947, rec: 0.7600, f1: 0.5027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:13<00:00, 10.54it/s, loss=0.00404] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002597281015362485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.13it/s, loss=0.00231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.004285948420917522\n",
      "Epoch 19 Sen Valid   acc: 0.9987, pre: 0.0564, rec: 0.0555, f1: 0.0543\n",
      "Epoch 19 Case Valid   acc: 0.9474, pre: 0.4006, rec: 0.7631, f1: 0.5079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410/1410 [02:14<00:00, 10.49it/s, loss=0.00025] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.002344912975751597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [01:54<00:00,  7.12it/s, loss=0.00255] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.004631572013122122\n",
      "Epoch 20 Sen Valid   acc: 0.9987, pre: 0.0566, rec: 0.0561, f1: 0.0547\n",
      "Epoch 20 Case Valid   acc: 0.9436, pre: 0.3857, rec: 0.7778, f1: 0.4979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from torch import logit\n",
    "\n",
    "\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+0.001)\n",
    "    r = TP / (TP + FN+0.001)\n",
    "    F1 = 2 * r * p / (r + p+0.0001)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "# def get_predict_label(logits,threshold):\n",
    "#     for i in range(len(logits)):\n",
    "#         for j in range(len(logits[0])):\n",
    "#             if logits[i][j]>threshold: logits[i][j] = 1\n",
    "#             else: logits[i][j] = 0\n",
    "#     return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    with tqdm(train_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            label = label.to(device)\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            logits = logits.cpu()\n",
    "            # t.set_postfix(loss=loss.item(),min=torch.min(logits),max=torch.max(logits),mean = torch.mean(logits))\n",
    "            t.set_postfix(loss=loss.item())\n",
    "        print('Train Loss:',running_loss/len(train_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "    running_loss =0\n",
    "    n=0\n",
    "    with tqdm(valid_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, c_label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "            n+=len(id)\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            threshold = 0.5\n",
    "            logits[logits>threshold] = 1\n",
    "            logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "            logits=logits.cpu().numpy()\n",
    "            # for i in range(logits.shape[0]):\n",
    "            #     c_predict=np.zeros(class_num)\n",
    "            #     c_predict[np.argmax(logits[i])]=1\n",
    "            #     logits[i]=c_predict\n",
    "            \n",
    "            # 单句标签f1\n",
    "            c_label = c_label.cpu().numpy()\n",
    "            for i in range(len(id)):\n",
    "                idx = int(id[i])\n",
    "                row_idx = valid_index_dict[idx]\n",
    "                predict[row_idx] += logits[i]\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(logits)):\n",
    "                acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "                # print(logits[0],label[1])       \n",
    "                total_precision+= p\n",
    "                total_recall+= r\n",
    "                total_f1+= F1\n",
    "                total_acc+= acc\n",
    "\n",
    "        # 案件标签f1\n",
    "        predict[predict>1] = 1\n",
    "        for i in range(predict.shape[0]):\n",
    "\n",
    "            acc,p,r,F1 = cal_metrics(predict[i],label[i]) \n",
    "            case_precision+= p\n",
    "            case_recall+= r\n",
    "            case_f1+= F1\n",
    "            case_acc+= acc\n",
    "        print('Valid Loss:',running_loss/len(valid_dataloader))\n",
    "            \n",
    "        print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "        print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
