{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from model import CaseClassification\n",
    "from transformers import BertTokenizer, AdamW,AutoModel, AutoTokenizer\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "if debug :\n",
    "    epochs = 2\n",
    "    train_batch = 2\n",
    "    valid_batch = 4\n",
    "else :\n",
    "    epochs = 5\n",
    "    train_batch = 32\n",
    "    valid_batch = 64\n",
    "\n",
    "learning_rate = 1e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 10,
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "model_path = \"thunlp/Lawformer\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 11,
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 16356\n",
      "正样本数: 1316\n",
      "负样本数: 15040\n",
      "正负样本比例: 0.0875\n",
      "(1447, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data_path = '../preprocessdata/'\n",
    "data = pd.read_csv(process_data_path+'train.csv')\n",
    "\n",
    "# drop nan text \n",
    "data=data[data[\"content\"].notnull()]\n",
    "\n",
    "vec_frame = pd.read_csv(process_data_path+'vec_frame.csv')\n",
    "if debug:\n",
    "    vec_frame = vec_frame.head(200)\n",
    "train_rate = 0.80\n",
    "case_size = vec_frame.shape[0]\n",
    "\n",
    "\n",
    "train_index_list = vec_frame.iloc[0:int(train_rate*case_size),0]\n",
    "valid_index_list = vec_frame.iloc[int(train_rate*case_size):,0]\n",
    "\n",
    "\n",
    "train_label = vec_frame[vec_frame[\"0\"].isin(train_index_list)]\n",
    "valid_label = vec_frame[vec_frame[\"0\"].isin(valid_index_list)]\n",
    "train_data = data[data[\"id\"].isin(train_index_list)]\n",
    "valid_data = data[data[\"id\"].isin(valid_index_list)]\n",
    "\n",
    "num = train_data.shape[0]\n",
    "p_num = train_data[train_data[\"label\"]!= '234'].shape[0]\n",
    "n_num = train_data[train_data[\"label\"]== '234'].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 1\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"label\"]=='234'].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"label\"]!='234']\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print('训练样本数：',train_data_now.shape)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 12,
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,index_list,class_num):\n",
    "        self.data = data\n",
    "        self.index_list = index_list\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,2]\n",
    "        id = int(self.data.iloc[idx,0])\n",
    "        label_list = self.data.iloc[idx,1]\n",
    "        label = torch.zeros(self.class_num)\n",
    "        for i in label_list.split(\"#\"):\n",
    "            label[int(i)] = 1\n",
    "        \n",
    "        return id,fact, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 235\n",
    "train_dataset = CaseData(train_data_now,train_index_list,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_index_list,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=4,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
<<<<<<< HEAD
=======
   "outputs": [],
   "source": [
    "# to check dataset\n",
    "# def check_dataset(data_set):\n",
    "#     for i in range(train_dataset.__len__()):\n",
    "#         a,b,c=train_dataset.__getitem__(i)  \n",
    "#         if not (isinstance (a,int) and isinstance(b,str) ):\n",
    "#             print(type(a),type(b),type(c),b)\n",
    "# check_dataset(valid_dataset)\n",
    "# print(valid_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Some weights of the model checkpoint at thunlp/Lawformer were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at thunlp/Lawformer and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
=======
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multilabel_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    https://kexue.fm/archives/7359\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred  # -1 -> pos classes, 1 -> neg classes\n",
    "    y_pred_neg = y_pred - y_true * 1e12  # mask the pred outputs of pos classes\n",
    "    y_pred_pos = (y_pred - (1 - y_true) * 1e12)  # mask the pred outputs of neg classes\n",
    "    zeros = torch.zeros_like(y_pred[..., :1])\n",
    "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "    return (neg_loss + pos_loss).mean()\n",
<<<<<<< HEAD
    "    \n",
=======
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
    "import torch.nn as nn\n",
    "class CaseClassification(nn.Module):\n",
    "    def __init__(self, class_num,model_path):\n",
    "        super(CaseClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        self.linear = nn.Linear(in_features=768, out_features=class_num)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, label=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_output = outputs['pooler_output']\n",
    "\n",
    "        logits = self.linear(pooler_output)\n",
    "\n",
    "        if label is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            # loss = loss_fn(logits, label)\n",
    "            loss = multilabel_cross_entropy(logits, label)\n",
    "            return loss, logits\n",
    "\n",
    "        return logits\n",
    "\n",
<<<<<<< HEAD
    "\n",
=======
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 234)\n",
      "[233747 233748 233749 233753 233754 233755 233757 233760 233763 233764\n",
      " 233769 233770 233776 233794 233799 233804 233805 233810 233813 233818\n",
      " 233819 233821 233822 233823 233824 233825 233826 233828 233829 233830\n",
      " 233831 233832 233834 233836 233839 233840 233844 233846 233848 233849]\n",
      "{233747: 0, 233748: 1, 233749: 2, 233753: 3, 233754: 4, 233755: 5, 233757: 6, 233760: 7, 233763: 8, 233764: 9, 233769: 10, 233770: 11, 233776: 12, 233794: 13, 233799: 14, 233804: 15, 233805: 16, 233810: 17, 233813: 18, 233818: 19, 233819: 20, 233821: 21, 233822: 22, 233823: 23, 233824: 24, 233825: 25, 233826: 26, 233828: 27, 233829: 28, 233830: 29, 233831: 30, 233832: 31, 233834: 32, 233836: 33, 233839: 34, 233840: 35, 233844: 36, 233846: 37, 233848: 38, 233849: 39}\n",
      "[[1 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "print(label.shape)\n",
    "print(valid_label.iloc[:,0].values)\n",
    "print(valid_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train, step: 50, train_loss: 6.215\n",
      "Epoch 1 Train, step: 100, train_loss: 6.076\n",
      "Epoch 1 Train, step: 150, train_loss: 5.935\n",
      "Epoch 1 Train, step: 200, train_loss: 5.884\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9080/3899451364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[0mvalid_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9080/3899451364.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprint_diff\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mprint_diff\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "batch_text_or_text_pairs has to be a list (got <class 'tuple'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11976/2052422177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11976/2052422177.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# tokenize the data text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m# move data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m                 )\n\u001b[1;32m   2494\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2496\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2684\u001b[0m         )\n\u001b[1;32m   2685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2687\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"batch_text_or_text_pairs has to be a list (got {type(batch_text_or_text_pairs)})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;31m# Set the truncation and padding strategy and restore the initial configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch_text_or_text_pairs has to be a list (got <class 'tuple'>)"
>>>>>>> fa19cf0129a767776a587f1f41ecb9eb44187922
     ]
    }
   ],
   "source": [
    "\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+1)\n",
    "    r = TP / (TP + FN+1)\n",
    "    F1 = 2 * r * p / (r + p+1)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN+1)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "# def get_predict_label(logits,threshold):\n",
    "#     for i in range(len(logits)):\n",
    "#         for j in range(len(logits[0])):\n",
    "#             if logits[i][j]>threshold: logits[i][j] = 1\n",
    "#             else: logits[i][j] = 0\n",
    "#     return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        id,fact, label= data\n",
    "\n",
    "        # tokenize the data text\n",
    "        inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        \n",
    "        # forward and backward propagations\n",
    "        loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % print_diff == print_diff -1 :\n",
    "            print('Epoch %d Train, step: %2d, train_loss: %.3f' % (epoch + 1, i + 1, running_loss / print_diff))\n",
    "            # wandb.log({'train_loss': running_loss / 50})\n",
    "            running_loss = 0.0\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "\n",
    "    \n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        id,fact, c_label= data\n",
    "\n",
    "        # tokenize the data text\n",
    "        inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # forward and backward propagations\n",
    "        loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "\n",
    "        threshold = 0\n",
    "        logits[logits>threshold] = 1\n",
    "        logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "        logits=logits.cpu().numpy()\n",
    "        # for i in range(logits.shape[0]):\n",
    "        #     c_predict=np.zeros(class_num)\n",
    "        #     c_predict[np.argmax(logits[i])]=1\n",
    "        #     logits[i]=c_predict\n",
    "        \n",
    "        # 单句标签f1\n",
    "        c_label = c_label.cpu().numpy()\n",
    "        for i in range(len(id)):\n",
    "            idx = int(id[i])\n",
    "            row_idx = valid_index_dict[idx]\n",
    "            predict[row_idx] += logits[i]\n",
    "        \n",
    "\n",
    "        for i in range(len(logits)):\n",
    "            acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "            # print(logits[0],label[1])       \n",
    "            total_precision+= p\n",
    "            total_recall+= r\n",
    "            total_f1+= F1\n",
    "            total_acc+= acc\n",
    "\n",
    "    # 案件标签f1\n",
    "    predict[predict>1] = 1\n",
    "    for i in range(predict.shape[0]):\n",
    "\n",
    "        acc,p,r,F1 = cal_metrics(predict[i][0:-1],label[i]) \n",
    "        case_precision+= p\n",
    "        case_recall+= r\n",
    "        case_f1+= F1\n",
    "        case_acc+= acc\n",
    "\n",
    "        \n",
    "    print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "    print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size,))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
