{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用苏神的loss去做，然后加上测试的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, AdamW,AutoModel, AutoTokenizer\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug :\n",
    "    epochs = 2\n",
    "    train_batch = 16\n",
    "    valid_batch = 4\n",
    "else :\n",
    "    epochs = 20\n",
    "    train_batch = 16\n",
    "    valid_batch = 64\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# model_path = \"thunlp/Lawformer\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 215833\n",
      "正样本数: 17480\n",
      "负样本数: 198353\n",
      "正负样本比例: 0.088125715265209\n",
      "训练样本数： (19228, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data_path = '../preprocessdata/'\n",
    "data = pd.read_csv(process_data_path+'train.csv')\n",
    "\n",
    "# drop nan text \n",
    "data=data[data[\"content\"].notnull()]\n",
    "\n",
    "vec_frame = pd.read_csv(process_data_path+'vec_frame.csv')\n",
    "if debug:\n",
    "    vec_frame = vec_frame.head(200)\n",
    "train_rate = 0.8\n",
    "case_size = vec_frame.shape[0]\n",
    "\n",
    "\n",
    "train_index_list = vec_frame.iloc[0:int(train_rate*case_size),0]\n",
    "valid_index_list = vec_frame.iloc[int(train_rate*case_size):,0]\n",
    "\n",
    "\n",
    "train_label = vec_frame[vec_frame[\"0\"].isin(train_index_list)]\n",
    "valid_label = vec_frame[vec_frame[\"0\"].isin(valid_index_list)]\n",
    "train_data = data[data[\"id\"].isin(train_index_list)]\n",
    "valid_data = data[data[\"id\"].isin(valid_index_list)]\n",
    "\n",
    "num = train_data.shape[0]\n",
    "p_num = train_data[train_data[\"label\"]!= '234'].shape[0]\n",
    "n_num = train_data[train_data[\"label\"]== '234'].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 10\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"label\"]=='234'].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"label\"]!='234']\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print('训练样本数：',train_data_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = []\n",
    "# for i in train_data_now[\"content\"]:\n",
    "#     c.append(len(i))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,index_list,class_num):\n",
    "        self.data = data\n",
    "        self.index_list = index_list\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,2]\n",
    "        id = int(self.data.iloc[idx,0])\n",
    "        label_list = self.data.iloc[idx,1]\n",
    "        label = torch.zeros(self.class_num)\n",
    "        for i in label_list.split(\"#\"):\n",
    "            if int(i) != 234:\n",
    "                label[int(i)] = 1\n",
    "        return id,fact, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 234\n",
    "train_dataset = CaseData(train_data_now,train_index_list,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_index_list,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multilabel_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    https://kexue.fm/archives/7359\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred  # -1 -> pos classes, 1 -> neg classes\n",
    "    y_pred_neg = y_pred - y_true * 1e12  # mask the pred outputs of pos classes\n",
    "    y_pred_pos = (y_pred - (1 - y_true) * 1e12)  # mask the pred outputs of neg classes\n",
    "    zeros = torch.zeros_like(y_pred[..., :1])\n",
    "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "    return (neg_loss + pos_loss).mean()\n",
    "    \n",
    "import torch.nn as nn\n",
    "class CaseClassification(nn.Module):\n",
    "    def __init__(self, class_num,model_path):\n",
    "        super(CaseClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        self.linear = nn.Linear(in_features=768, out_features=class_num)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, label=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_output = outputs['pooler_output']\n",
    "\n",
    "        logits = self.linear(pooler_output)\n",
    "        # logits = torch.sigmoid(logits)\n",
    "        if label is not None:\n",
    "            loss_fn = nn.BCELoss()\n",
    "            # loss_fn = nn.BCEWithLogitsLoss()\n",
    "            # loss = loss_fn(logits, label)\n",
    "            loss = multilabel_cross_entropy(logits, label)\n",
    "            return loss, logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = valid_label.values[:,1:]\n",
    "# valid_size = valid_data.shape[0]\n",
    "# valid_case_size = len(valid_label)\n",
    "# valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "# print(label.shape)\n",
    "# print(valid_label.iloc[:,0].values)\n",
    "# print(valid_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:54<00:00, 10.48it/s, loss=3.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.280588933314737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:26<00:00,  7.30it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.9574645374821367\n",
      "Epoch 1 Sen Valid   acc: 0.9987, pre: 0.0437, rec: 0.0405, f1: 0.0407\n",
      "Epoch 1 Case Valid   acc: 0.9586, pre: 0.4341, rec: 0.5376, f1: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:55<00:00, 10.44it/s, loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8829375002626176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:27<00:00,  7.22it/s, loss=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.245943907829706\n",
      "Epoch 2 Sen Valid   acc: 0.9987, pre: 0.0496, rec: 0.0451, f1: 0.0460\n",
      "Epoch 2 Case Valid   acc: 0.9537, pre: 0.4059, rec: 0.6245, f1: 0.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:56<00:00, 10.32it/s, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2982808113494846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:26<00:00,  7.27it/s, loss=0.653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.9116123065296631\n",
      "Epoch 3 Sen Valid   acc: 0.9988, pre: 0.0524, rec: 0.0473, f1: 0.0484\n",
      "Epoch 3 Case Valid   acc: 0.9536, pre: 0.4128, rec: 0.6528, f1: 0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:56<00:00, 10.33it/s, loss=2.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.93481775612482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:27<00:00,  7.26it/s, loss=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8436735365330503\n",
      "Epoch 4 Sen Valid   acc: 0.9986, pre: 0.0554, rec: 0.0511, f1: 0.0518\n",
      "Epoch 4 Case Valid   acc: 0.9387, pre: 0.3829, rec: 0.7244, f1: 0.4773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:56<00:00, 10.36it/s, loss=1.98] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6624562277670907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:27<00:00,  7.26it/s, loss=0.601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8830346942003746\n",
      "Epoch 5 Sen Valid   acc: 0.9985, pre: 0.0561, rec: 0.0534, f1: 0.0532\n",
      "Epoch 5 Case Valid   acc: 0.9454, pre: 0.3732, rec: 0.7390, f1: 0.4795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:55<00:00, 10.39it/s, loss=1.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4500481783104022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:26<00:00,  7.28it/s, loss=0.645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8673464320739557\n",
      "Epoch 6 Sen Valid   acc: 0.9985, pre: 0.0569, rec: 0.0533, f1: 0.0536\n",
      "Epoch 6 Case Valid   acc: 0.9422, pre: 0.3656, rec: 0.7488, f1: 0.4733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:56<00:00, 10.36it/s, loss=0.908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.277398832576247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:27<00:00,  7.26it/s, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.9093429404293442\n",
      "Epoch 7 Sen Valid   acc: 0.9985, pre: 0.0575, rec: 0.0551, f1: 0.0547\n",
      "Epoch 7 Case Valid   acc: 0.9425, pre: 0.3634, rec: 0.7722, f1: 0.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:56<00:00, 10.36it/s, loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1208115906366294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:28<00:00,  7.21it/s, loss=0.586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8847598179770209\n",
      "Epoch 8 Sen Valid   acc: 0.9985, pre: 0.0576, rec: 0.0559, f1: 0.0551\n",
      "Epoch 8 Case Valid   acc: 0.9424, pre: 0.3644, rec: 0.7778, f1: 0.4801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1202/1202 [01:56<00:00, 10.29it/s, loss=1.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9922100249151025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1068/1068 [02:28<00:00,  7.21it/s, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8828710205852985\n",
      "Epoch 9 Sen Valid   acc: 0.9986, pre: 0.0575, rec: 0.0552, f1: 0.0547\n",
      "Epoch 9 Case Valid   acc: 0.9407, pre: 0.3558, rec: 0.7744, f1: 0.4711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 645/1202 [01:03<00:54, 10.19it/s, loss=0.446]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_149800/816402758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_149800/816402758.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+0.001)\n",
    "    r = TP / (TP + FN+0.001)\n",
    "    F1 = 2 * r * p / (r + p+0.0001)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "# def get_predict_label(logits,threshold):\n",
    "#     for i in range(len(logits)):\n",
    "#         for j in range(len(logits[0])):\n",
    "#             if logits[i][j]>threshold: logits[i][j] = 1\n",
    "#             else: logits[i][j] = 0\n",
    "#     return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    with tqdm(train_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            label = label.to(device)\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            logits = logits.cpu()\n",
    "            # t.set_postfix(loss=loss.item(),min=torch.min(logits),max=torch.max(logits),mean = torch.mean(logits))\n",
    "            t.set_postfix(loss=loss.item())\n",
    "        print('Train Loss:',running_loss/len(train_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "    running_loss =0\n",
    "    n=0\n",
    "    with tqdm(valid_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, c_label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "            n+=len(id)\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            threshold = 0\n",
    "            logits[logits>threshold] = 1\n",
    "            logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "            logits=logits.cpu().numpy()\n",
    "            # for i in range(logits.shape[0]):\n",
    "            #     c_predict=np.zeros(class_num)\n",
    "            #     c_predict[np.argmax(logits[i])]=1\n",
    "            #     logits[i]=c_predict\n",
    "            \n",
    "            # 单句标签f1\n",
    "            c_label = c_label.cpu().numpy()\n",
    "            for i in range(len(id)):\n",
    "                idx = int(id[i])\n",
    "                row_idx = valid_index_dict[idx]\n",
    "                predict[row_idx] += logits[i]\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(logits)):\n",
    "                acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "                # print(logits[0],label[1])       \n",
    "                total_precision+= p\n",
    "                total_recall+= r\n",
    "                total_f1+= F1\n",
    "                total_acc+= acc\n",
    "\n",
    "        # 案件标签f1\n",
    "        predict[predict>1] = 1\n",
    "        for i in range(predict.shape[0]):\n",
    "\n",
    "            acc,p,r,F1 = cal_metrics(predict[i],label[i]) \n",
    "            case_precision+= p\n",
    "            case_recall+= r\n",
    "            case_f1+= F1\n",
    "            case_acc+= acc\n",
    "        print('Valid Loss:',running_loss/len(valid_dataloader))\n",
    "            \n",
    "        print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "        print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 保存模型\n",
    "torch.save({'model': model.state_dict()}, 'model_name.pth')\n",
    "\n",
    "## 读取模型\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "state_dict = torch.load('model_name.pth')\n",
    "model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils import get_fact, get_level3labels\n",
    "input_path = \"../rawdata/train.json\"\n",
    "output_path = \"./output/result_train.txt\"\n",
    "level3labels = get_level3labels(json.load(open('../rawdata/code_tree.json')))\n",
    "\n",
    "records = json.load(open(input_path, encoding='utf-8'))\n",
    "records = records[0:2]\n",
    "results = []\n",
    "for record in tqdm(records):\n",
    "    fact_list = record['content']\n",
    "    result = set()\n",
    "    for fact in fact_list:\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        _, logits = model(input_ids, attention_mask, token_type_ids, torch.zeros(234).to(device))\n",
    "\n",
    "\n",
    "        threshold = 0.5\n",
    "        logits = logits[0]\n",
    "        logits[logits>threshold] = 1\n",
    "        logits[logits<=threshold] = 0\n",
    "\n",
    "        \n",
    "        for p in range(len(logits)):\n",
    "            if logits[p]==1:\n",
    "                result.add(level3labels[p])\n",
    "    result=list(result)\n",
    "    results.append(result)\n",
    "json.dump(results, open(output_path, \"w\", encoding=\"utf8\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
