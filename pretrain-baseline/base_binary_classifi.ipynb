{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, AdamW,AutoModel, AutoTokenizer\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug :\n",
    "    epochs = 2\n",
    "    train_batch = 2\n",
    "    valid_batch = 4\n",
    "else :\n",
    "    epochs = 5\n",
    "    train_batch = 16\n",
    "    valid_batch = 64\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# model_path = \"thunlp/Lawformer\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 215833\n",
      "正样本数: 17480\n",
      "负样本数: 198353\n",
      "正负样本比例: 0.088125715265209\n",
      "训练样本数： (18354, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_data_path = '../preprocessdata/'\n",
    "data = pd.read_csv(process_data_path+'train.csv')\n",
    "\n",
    "# drop nan text \n",
    "data=data[data[\"content\"].notnull()]\n",
    "\n",
    "vec_frame = pd.read_csv(process_data_path+'vec_frame.csv')\n",
    "if debug:\n",
    "    vec_frame = vec_frame.head(20)\n",
    "train_rate = 0.80\n",
    "case_size = vec_frame.shape[0]\n",
    "\n",
    "\n",
    "train_index_list = vec_frame.iloc[0:int(train_rate*case_size),0]\n",
    "valid_index_list = vec_frame.iloc[int(train_rate*case_size):,0]\n",
    "\n",
    "\n",
    "train_label = vec_frame[vec_frame[\"0\"].isin(train_index_list)]\n",
    "valid_label = vec_frame[vec_frame[\"0\"].isin(valid_index_list)]\n",
    "train_data = data[data[\"id\"].isin(train_index_list)]\n",
    "valid_data = data[data[\"id\"].isin(valid_index_list)]\n",
    "\n",
    "num = train_data.shape[0]\n",
    "p_num = train_data[train_data[\"label\"]!= '234'].shape[0]\n",
    "n_num = train_data[train_data[\"label\"]== '234'].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 20\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"label\"]=='234'].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"label\"]!='234']\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print('训练样本数：',train_data_now.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = []\n",
    "# for i in train_data_now[\"content\"]:\n",
    "#     c.append(len(i))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,index_list,class_num):\n",
    "        self.data = data\n",
    "        self.index_list = index_list\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,2]\n",
    "        id = int(self.data.iloc[idx,0])\n",
    "        label_list = self.data.iloc[idx,1]\n",
    "        label = torch.zeros(self.class_num)\n",
    "        for i in label_list.split(\"#\"):\n",
    "            if int(i) != 234:\n",
    "                label[int(i)] = 1\n",
    "        return id,fact, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 234\n",
    "train_dataset = CaseData(train_data_now,train_index_list,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_index_list,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multilabel_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    https://kexue.fm/archives/7359\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred  # -1 -> pos classes, 1 -> neg classes\n",
    "    y_pred_neg = y_pred - y_true * 1e12  # mask the pred outputs of pos classes\n",
    "    y_pred_pos = (y_pred - (1 - y_true) * 1e12)  # mask the pred outputs of neg classes\n",
    "    zeros = torch.zeros_like(y_pred[..., :1])\n",
    "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "    return (neg_loss + pos_loss).mean()\n",
    "    \n",
    "\n",
    "# class CaseClassification(nn.Module):\n",
    "#     def __init__(self, class_num,model_path):\n",
    "#         super(CaseClassification, self).__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(model_path)\n",
    "#         self.linear = nn.Linear(in_features=768, out_features=class_num)\n",
    "\n",
    "#     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, label=None):\n",
    "#         outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "#         pooler_output = outputs['pooler_output']\n",
    "\n",
    "#         logits = self.linear(pooler_output)\n",
    "\n",
    "#         if label is not None:\n",
    "#             loss_fn = nn.BCEWithLogitsLoss()\n",
    "#             # loss = loss_fn(logits, label)\n",
    "#             loss = multilabel_cross_entropy(logits, label)\n",
    "#             return loss, logits\n",
    "\n",
    "#         return logits\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert: BertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                \n",
    "            labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "        cls_output = outputs[1] # batch, hidden\n",
    "        cls_output = self.classifier(cls_output) # batch, 6\n",
    "        cls_output = torch.sigmoid(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels)\n",
    "        return loss, cls_output\n",
    "\n",
    "model = BertClassifier(BertModel.from_pretrained(model_path), class_num).to(device)\n",
    "\n",
    "# load the model and tokenizer\n",
    "# model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = valid_label.values[:,1:]\n",
    "# valid_size = valid_data.shape[0]\n",
    "# valid_case_size = len(valid_label)\n",
    "# valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "# print(label.shape)\n",
    "# print(valid_label.iloc[:,0].values)\n",
    "# print(valid_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [01:51<00:00, 10.33it/s, loss=0.0174] \n",
      "100%|██████████| 1068/1068 [02:26<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Sen Valid   acc: 0.9911, pre: 0.0058, rec: 0.0052, f1: 0.0027\n",
      "Epoch 1 Case Valid   acc: 0.9604, pre: 0.3250, rec: 0.0799, f1: 0.0484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1148/1148 [01:51<00:00, 10.31it/s, loss=0.0099] \n",
      "100%|██████████| 1068/1068 [02:26<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Sen Valid   acc: 0.9911, pre: 0.0058, rec: 0.0052, f1: 0.0027\n",
      "Epoch 2 Case Valid   acc: 0.9604, pre: 0.3250, rec: 0.0799, f1: 0.0484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 51/1148 [00:04<01:45, 10.42it/s, loss=0.00176]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115321/3783217484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_115321/3783217484.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# tokenize the data text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# move data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m                 )\n\u001b[1;32m   2494\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2495\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2496\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2684\u001b[0m         )\n\u001b[1;32m   2685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2687\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# words in the English Wikipedia.).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_chinese_chars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_chinese_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0morig_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize_chinese_chars\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+1)\n",
    "    r = TP / (TP + FN+1)\n",
    "    F1 = 2 * r * p / (r + p+1)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN+1)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "# def get_predict_label(logits,threshold):\n",
    "#     for i in range(len(logits)):\n",
    "#         for j in range(len(logits[0])):\n",
    "#             if logits[i][j]>threshold: logits[i][j] = 1\n",
    "#             else: logits[i][j] = 0\n",
    "#     return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "    n = 0\n",
    "    with tqdm(train_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, label= data\n",
    "            \n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            label = label.to(device)\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, labels=label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            t.set_postfix(loss=running_loss/len(id))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "    \n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "\n",
    "    with tqdm(valid_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, c_label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, labels=c_label)\n",
    "            \n",
    "            threshold = 0.1\n",
    "            # print(logits)\n",
    "            logits[logits>threshold] = 1\n",
    "            logits[logits<=threshold] = 0\n",
    "\n",
    "            \n",
    "\n",
    "            logits=logits.cpu().numpy()\n",
    "            # for i in range(logits.shape[0]):\n",
    "            #     c_predict=np.zeros(class_num)\n",
    "            #     c_predict[np.argmax(logits[i])]=1\n",
    "            #     logits[i]=c_predict\n",
    "            \n",
    "            # 单句标签f1\n",
    "            c_label = c_label.cpu().numpy()\n",
    "            for i in range(len(id)):\n",
    "                idx = int(id[i])\n",
    "                row_idx = valid_index_dict[idx]\n",
    "                predict[row_idx] += logits[i]\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(logits)):\n",
    "                acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "                # print(logits[0],label[1])       \n",
    "                total_precision+= p\n",
    "                total_recall+= r\n",
    "                total_f1+= F1\n",
    "                total_acc+= acc\n",
    "\n",
    "        # 案件标签f1\n",
    "        predict[predict>1] = 1\n",
    "        for i in range(predict.shape[0]):\n",
    "\n",
    "            acc,p,r,F1 = cal_metrics(predict[i],label[i]) \n",
    "            case_precision+= p\n",
    "            case_recall+= r\n",
    "            case_f1+= F1\n",
    "            case_acc+= acc\n",
    "\n",
    "            \n",
    "        print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "        print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size,))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
