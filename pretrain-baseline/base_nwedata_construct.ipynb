{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, AdamW,AutoModel, AutoTokenizer\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "debug_train_num = 100\n",
    "debug_valid_num = 20\n",
    "train_batch = 16\n",
    "valid_batch = 64\n",
    "model_path = 'bert-base-chinese'   #'chinese-bert-wwm',\n",
    "learning_rate = 5e-5\n",
    "train_rate = 0.8\n",
    "content_size = 100\n",
    "process_data_path = '../processeddata/tr-%s-%s/' %(str(train_rate),str(content_size))\n",
    "epoch_number = 10\n",
    "freeze = True\n",
    "loss= 'MCE' # BCE、BCEWG\n",
    "optm = 'Adam'\n",
    "pn_rate = 1\n",
    "\n",
    "class_num = 234\n",
    "\n",
    "time_limit = 30\n",
    "f1_limit = 0.55\n",
    "diff_limit =2\n",
    "f1_save_limit = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()  \n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# model_path = \"thunlp/Lawformer\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167879, 236)\n",
      "(42489, 236)\n",
      "(1997, 235)\n",
      "(499, 235)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data=[onehot-vec,id,content]\n",
    "label = [0,onehot-vec]\n",
    "'''\n",
    "\n",
    "process_data_path = '../processeddata/tr-0.8-100/'\n",
    "train_data = pd.read_csv(process_data_path+'train_data.csv')\n",
    "valid_data = pd.read_csv(process_data_path+'valid_data.csv')\n",
    "train_label = pd.read_csv(process_data_path+'train_label.csv')\n",
    "valid_label = pd.read_csv(process_data_path+'valid_label.csv')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147609, 236)\n",
      "(37241, 236)\n",
      "(1997, 235)\n",
      "(499, 235)\n"
     ]
    }
   ],
   "source": [
    "train_data=train_data[train_data.iloc[:,-1].notnull()]\n",
    "valid_data=valid_data[valid_data.iloc[:,-1].notnull()]\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8195, 236)\n",
      "(1318, 236)\n",
      "(100, 235)\n",
      "(20, 235)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if debug :\n",
    "    train_label=train_label.head(debug_train_num)\n",
    "    valid_label=valid_label.head(debug_valid_num)\n",
    "    train_data=train_data[train_data[\"id\"].isin(train_label.iloc[:,0])]\n",
    "    valid_data=valid_data[valid_data[\"id\"].isin(valid_label.iloc[:,0])]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8195/8195 [00:02<00:00, 3223.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 8195\n",
      "正样本数: 1163\n",
      "负样本数: 7032\n",
      "正负样本比例: 0.165386803185438\n",
      "训练样本数： (2326, 237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num = train_data.shape[0]\n",
    "train_data_sum = []\n",
    "for i in tqdm(range(num)):\n",
    "    train_data_sum.append(train_data.iloc[i,0:234].sum())\n",
    "train_data['sum']=train_data_sum\n",
    "\n",
    "p_num = train_data[train_data[\"sum\"]!= 0].shape[0]\n",
    "n_num = train_data[train_data[\"sum\"]== 0].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"sum\"]==0].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"sum\"]!=0]\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print('训练样本数：',train_data_now.shape)\n",
    "del train_data_now['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,label,class_num):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,-1]\n",
    "        id = int(self.data.iloc[idx,-2])\n",
    "        l = torch.tensor(self.data.iloc[idx,0:class_num], dtype=float)\n",
    "        # print(fact,id,l)\n",
    "\n",
    "        return id,fact, l\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "train_dataset = CaseData(train_data_now,train_label,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_label,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multilabel_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    https://kexue.fm/archives/7359\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred  # -1 -> pos classes, 1 -> neg classes\n",
    "    y_pred_neg = y_pred - y_true * 1e12  # mask the pred outputs of pos classes\n",
    "    y_pred_pos = (y_pred - (1 - y_true) * 1e12)  # mask the pred outputs of neg classes\n",
    "    zeros = torch.zeros_like(y_pred[..., :1])\n",
    "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "    return (neg_loss + pos_loss).mean()\n",
    "    \n",
    "import torch.nn as nn\n",
    "class CaseClassification(nn.Module):\n",
    "    def __init__(self, class_num,model_path):\n",
    "        super(CaseClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        self.linear = nn.Linear(in_features=768, out_features=class_num)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, label=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_output = outputs['pooler_output']\n",
    "\n",
    "        logits = self.linear(pooler_output)\n",
    "        # logits = torch.sigmoid(logits)\n",
    "        if label is not None:\n",
    "            loss_fn = nn.BCELoss()\n",
    "            # loss_fn = nn.BCEWithLogitsLoss()\n",
    "            # loss = loss_fn(logits, label)\n",
    "            loss = multilabel_cross_entropy(logits, label)\n",
    "            return loss, logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "if freeze:\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "else:\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "  \n",
    "def out_time_limit(start,limit_minutes):\n",
    "    now = datetime.datetime.now()  \n",
    "    result = (now - start).total_seconds()\n",
    "    if result>60*limit_minutes:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:10<00:00, 14.29it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4208469507480035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  9.07it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.3625684131984834\n",
      "Epoch 1 Sen Valid   acc: 0.9990, pre: 0.0212, rec: 0.0163, f1: 0.0179\n",
      "Epoch 1 Case Valid   acc: 0.9697, pre: 0.5746, rec: 0.1746, f1: 0.2465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:10<00:00, 14.36it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.953797108520918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  9.08it/s, loss=0.988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.2358036530087277\n",
      "Epoch 2 Sen Valid   acc: 0.9989, pre: 0.0292, rec: 0.0225, f1: 0.0244\n",
      "Epoch 2 Case Valid   acc: 0.9707, pre: 0.6146, rec: 0.2213, f1: 0.3039\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:10<00:00, 14.26it/s, loss=0.621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6156359308772572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  9.10it/s, loss=1.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.19704437633732\n",
      "Epoch 3 Sen Valid   acc: 0.9986, pre: 0.0466, rec: 0.0344, f1: 0.0378\n",
      "Epoch 3 Case Valid   acc: 0.9673, pre: 0.4668, rec: 0.3191, f1: 0.3625\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:10<00:00, 14.16it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.357833291349654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  9.12it/s, loss=0.94] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.0141371644715096\n",
      "Epoch 4 Sen Valid   acc: 0.9988, pre: 0.0432, rec: 0.0334, f1: 0.0360\n",
      "Epoch 4 Case Valid   acc: 0.9688, pre: 0.5971, rec: 0.2843, f1: 0.3643\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:10<00:00, 14.34it/s, loss=0.475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.168808330631444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  8.57it/s, loss=0.788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.0056414034719494\n",
      "Epoch 5 Sen Valid   acc: 0.9989, pre: 0.0517, rec: 0.0416, f1: 0.0441\n",
      "Epoch 5 Case Valid   acc: 0.9718, pre: 0.6290, rec: 0.4018, f1: 0.4536\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:16<00:00,  9.11it/s, loss=0.973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0149304815649403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:03<00:00,  6.67it/s, loss=0.823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.0078318156013852\n",
      "Epoch 6 Sen Valid   acc: 0.9989, pre: 0.0571, rec: 0.0433, f1: 0.0473\n",
      "Epoch 6 Case Valid   acc: 0.9718, pre: 0.5865, rec: 0.4113, f1: 0.4465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:16<00:00,  8.76it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8832058238203173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:03<00:00,  6.86it/s, loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 1.0594329341054987\n",
      "Epoch 7 Sen Valid   acc: 0.9987, pre: 0.0600, rec: 0.0484, f1: 0.0509\n",
      "Epoch 7 Case Valid   acc: 0.9690, pre: 0.5466, rec: 0.4445, f1: 0.4614\n",
      "save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 135/146 [00:15<00:01,  8.56it/s, loss=0.585]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115033/3466140593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"123456\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mnow_f1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnow_f1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mbest_f1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_115033/3466140593.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "valid_index_list=valid_label.values[:,0]\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+0.001)\n",
    "    r = TP / (TP + FN+0.001)\n",
    "    F1 = 2 * r * p / (r + p+0.0001)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "# def get_predict_label(logits,threshold):\n",
    "#     for i in range(len(logits)):\n",
    "#         for j in range(len(logits[0])):\n",
    "#             if logits[i][j]>threshold: logits[i][j] = 1\n",
    "#             else: logits[i][j] = 0\n",
    "#     return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    with tqdm(train_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            label = label.to(device)\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            logits = logits.cpu()\n",
    "            # t.set_postfix(loss=loss.item(),min=torch.min(logits),max=torch.max(logits),mean = torch.mean(logits))\n",
    "            t.set_postfix(loss=loss.item())\n",
    "        print('Train Loss:',running_loss/len(train_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "    running_loss =0\n",
    "    n=0\n",
    "    with tqdm(valid_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, c_label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "            n+=len(id)\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            threshold = 0\n",
    "            logits[logits>threshold] = 1\n",
    "            logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "            logits=logits.cpu().numpy()\n",
    "            # for i in range(logits.shape[0]):\n",
    "            #     c_predict=np.zeros(class_num)\n",
    "            #     c_predict[np.argmax(logits[i])]=1\n",
    "            #     logits[i]=c_predict\n",
    "            \n",
    "            # 单句标签f1\n",
    "            c_label = c_label.cpu().numpy()\n",
    "            for i in range(len(id)):\n",
    "                idx = int(id[i])\n",
    "                row_idx = valid_index_dict[idx]\n",
    "                predict[row_idx] += logits[i]\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(logits)):\n",
    "                acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "                # print(logits[0],label[1])       \n",
    "                total_precision+= p\n",
    "                total_recall+= r\n",
    "                total_f1+= F1\n",
    "                total_acc+= acc\n",
    "\n",
    "        # 案件标签f1\n",
    "        predict[predict>1] = 1\n",
    "        for i in range(predict.shape[0]):\n",
    "\n",
    "            acc,p,r,F1 = cal_metrics(predict[i],label[i]) \n",
    "            case_precision+= p\n",
    "            case_recall+= r\n",
    "            case_f1+= F1\n",
    "            case_acc+= acc\n",
    "        print('Valid Loss:',running_loss/len(valid_dataloader))\n",
    "            \n",
    "        print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "        print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size))\n",
    "        return case_f1/valid_case_size\n",
    "\n",
    "def model_save(model,model_name):\n",
    "    ## 保存模型\n",
    "    torch.save(model, './saved/%s.pth' % (model_name)) \n",
    "    print('save')\n",
    "\n",
    "best_f1 = 0\n",
    "model_name=hashlib.md5(\"123456\".encode(\"utf-8\")).hexdigest()\n",
    "for epoch in range(epoch_number):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    now_f1=valid_fn(valid_dataloader,epoch)\n",
    "    if now_f1>best_f1:\n",
    "        diff = 0\n",
    "        best_f1 = now_f1\n",
    "        if now_f1>f1_save_limit:\n",
    "            model_save(model,model_name)\n",
    "        if now_f1>f1_limit:          \n",
    "            break\n",
    "    else:\n",
    "        diff+=1\n",
    "        if diff>=diff_limit :\n",
    "            break\n",
    "        if out_time_limit(start,time_limit):\n",
    "            break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115033/749360562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils import get_fact, get_level3labels\n",
    "input_path = \"../rawdata/train.json\"\n",
    "output_path = \"./output/result_train.txt\"\n",
    "level3labels = get_level3labels(json.load(open('../rawdata/code_tree.json')))\n",
    "\n",
    "records = json.load(open(input_path, encoding='utf-8'))\n",
    "records = records[0:2]\n",
    "results = []\n",
    "for record in tqdm(records):\n",
    "    fact_list = record['content']\n",
    "    result = set()\n",
    "    for fact in fact_list:\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        _, logits = model(input_ids, attention_mask, token_type_ids, torch.zeros(234).to(device))\n",
    "\n",
    "\n",
    "        threshold = 0.5\n",
    "        logits = logits[0]\n",
    "        logits[logits>threshold] = 1\n",
    "        logits[logits<=threshold] = 0\n",
    "\n",
    "        \n",
    "        for p in range(len(logits)):\n",
    "            if logits[p]==1:\n",
    "                result.add(level3labels[p])\n",
    "    result=list(result)\n",
    "    results.append(result)\n",
    "json.dump(results, open(output_path, \"w\", encoding=\"utf8\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
