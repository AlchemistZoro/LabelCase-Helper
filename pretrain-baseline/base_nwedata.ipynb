{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用苏神的loss去做，然后加上测试的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, AdamW,AutoModel, AutoTokenizer\n",
    "import numpy as np \n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader,random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_path = 'E:/pre-train-model' # model root path on windows\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    'bert-base-chinese',\n",
    "    'chinese-bert-wwm',\n",
    "    \n",
    "]\n",
    "model_idx =0\n",
    "model_path = model_root_path + '/'+model_list[model_idx]+'/'\n",
    "# model download from hugging-face\n",
    "model_path = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "# model_path = \"thunlp/Lawformer\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150267, 236)\n",
      "(37253, 236)\n",
      "(1997, 235)\n",
      "(499, 235)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "data=[onehot-vec,id,content]\n",
    "label = [0,onehot-vec]\n",
    "'''\n",
    "\n",
    "process_data_path = '../processeddata/tr-0.8/'\n",
    "train_data = pd.read_csv(process_data_path+'train_data.csv')\n",
    "valid_data = pd.read_csv(process_data_path+'valid_data.csv')\n",
    "train_label = pd.read_csv(process_data_path+'train_label.csv')\n",
    "valid_label = pd.read_csv(process_data_path+'valid_label.csv')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150249, 236)\n",
      "(37253, 236)\n",
      "(1997, 235)\n",
      "(499, 235)\n"
     ]
    }
   ],
   "source": [
    "train_data=train_data[train_data.iloc[:,-1].notnull()]\n",
    "valid_label=valid_label[valid_label.iloc[:,-1].notnull()]\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7360, 236)\n",
      "(1364, 236)\n",
      "(100, 235)\n",
      "(20, 235)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if debug :\n",
    "    epochs = 5\n",
    "    train_batch = 16\n",
    "    valid_batch = 64\n",
    "    train_label=train_label.head(100)\n",
    "    valid_label=valid_label.head(20)\n",
    "    train_data=train_data[train_data[\"id\"].isin(train_label.iloc[:,0])]\n",
    "    valid_data=valid_data[valid_data[\"id\"].isin(valid_label.iloc[:,0])]\n",
    "else :\n",
    "    epochs = 20\n",
    "    train_batch = 16\n",
    "    valid_batch = 64\n",
    "\n",
    "learning_rate = 5e-5\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7360/7360 [00:02<00:00, 3262.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数: 7360\n",
      "正样本数: 3084\n",
      "负样本数: 4276\n",
      "正负样本比例: 0.7212347988774556\n",
      "训练样本数： (7360, 237)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num = train_data.shape[0]\n",
    "train_data_sum = []\n",
    "for i in tqdm(range(num)):\n",
    "    # print(train_data.iloc[i,0:234].sum())\n",
    "    train_data_sum.append(train_data.iloc[i,0:234].sum())\n",
    "train_data['sum']=train_data_sum\n",
    "\n",
    "p_num = train_data[train_data[\"sum\"]!= 0].shape[0]\n",
    "n_num = train_data[train_data[\"sum\"]== 0].shape[0]\n",
    "print('样本数:',num)\n",
    "print('正样本数:',p_num)\n",
    "print('负样本数:',n_num)\n",
    "print('正负样本比例:',p_num/n_num)\n",
    "pn_rate = 0.000001\n",
    "if pn_rate<p_num/n_num:\n",
    "    n_change_num = n_num\n",
    "else:\n",
    "    n_change_num = int(p_num/pn_rate)\n",
    "\n",
    "n_train_data=train_data[train_data[\"sum\"]==0].sample(n=n_change_num,replace=True)\n",
    "p_train_data=train_data[train_data[\"sum\"]!=0]\n",
    "train_data_now = pd.concat([n_train_data,p_train_data],axis=0)\n",
    "\n",
    "print('训练样本数：',train_data_now.shape)\n",
    "del train_data_now['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CaseData(Dataset):\n",
    "    def __init__(self, data,label,class_num):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.class_num = class_num\n",
    " \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fact = self.data.iloc[idx,-1]\n",
    "        id = int(self.data.iloc[idx,-2])\n",
    "        l = torch.tensor(self.data.iloc[idx,0:class_num], dtype=float)\n",
    "        # print(fact,id,l)\n",
    "\n",
    "        return id,fact, l\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class_num = 234\n",
    "train_dataset = CaseData(train_data_now,train_label,class_num=class_num)\n",
    "valid_dataset = CaseData(valid_data,valid_label,class_num=class_num)\n",
    "\n",
    "# print(len(full_data))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multilabel_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    https://kexue.fm/archives/7359\n",
    "    \"\"\"\n",
    "    y_pred = (1 - 2 * y_true) * y_pred  # -1 -> pos classes, 1 -> neg classes\n",
    "    y_pred_neg = y_pred - y_true * 1e12  # mask the pred outputs of pos classes\n",
    "    y_pred_pos = (y_pred - (1 - y_true) * 1e12)  # mask the pred outputs of neg classes\n",
    "    zeros = torch.zeros_like(y_pred[..., :1])\n",
    "    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)\n",
    "    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)\n",
    "    neg_loss = torch.logsumexp(y_pred_neg, dim=-1)\n",
    "    pos_loss = torch.logsumexp(y_pred_pos, dim=-1)\n",
    "\n",
    "    return (neg_loss + pos_loss).mean()\n",
    "    \n",
    "import torch.nn as nn\n",
    "class CaseClassification(nn.Module):\n",
    "    def __init__(self, class_num,model_path):\n",
    "        super(CaseClassification, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "        self.linear = nn.Linear(in_features=768, out_features=class_num)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, label=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        pooler_output = outputs['pooler_output']\n",
    "\n",
    "        logits = self.linear(pooler_output)\n",
    "        # logits = torch.sigmoid(logits)\n",
    "        if label is not None:\n",
    "            loss_fn = nn.BCELoss()\n",
    "            # loss_fn = nn.BCEWithLogitsLoss()\n",
    "            # loss = loss_fn(logits, label)\n",
    "            loss = multilabel_cross_entropy(logits, label)\n",
    "            return loss, logits\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# load the model and tokenizer\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare the optimizer and corresponding hyper-parameters\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([233051, 233082, 233089, 233091, 233096, 233100, 233117, 233157,\n",
       "       233199, 233266, 233275, 233434, 233511, 233522, 233560, 233562,\n",
       "       233563, 233592, 233609, 233614, 233619, 233635, 233638, 233642,\n",
       "       233646, 233651, 233656, 233658, 233693, 233695, 233707, 233749,\n",
       "       233755, 233769, 233821, 233823, 233826, 233832, 233839, 233858])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label = valid_label.values[:,1:]\n",
    "# valid_size = valid_data.shape[0]\n",
    "# valid_case_size = len(valid_label)\n",
    "# valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "# print(label.shape)\n",
    "# print(valid_label.iloc[:,0].values)\n",
    "# print(valid_index_dict)\n",
    "valid_index_list=valid_label.values[:,0]\n",
    "valid_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15999,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"content\"].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 116/1000 [00:37<04:46,  3.08it/s, loss=4.09]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63802/3418030976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_63802/3418030976.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataloader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_index_list=valid_label.values[:,0]\n",
    "label = valid_label.values[:,1:]\n",
    "valid_size = valid_data.shape[0]\n",
    "valid_case_size = len(valid_label)\n",
    "valid_index_dict = dict(zip(valid_index_list,range(len(valid_index_list))))\n",
    "\n",
    "def cal_metrics(pred_choice,target):\n",
    "    TP,TN,FN,FP = 0,0,0,0\n",
    "    # TP predict 和 label 同时为1\n",
    "    TP += ((pred_choice == 1) & (target == 1)).sum()\n",
    "    # TN predict 和 label 同时为0\n",
    "    TN += ((pred_choice == 0) & (target == 0)).sum()\n",
    "    # FN predict 0 label 1\n",
    "    FN += ((pred_choice == 0) & (target == 1)).sum()\n",
    "    # FP predict 1 label 0\n",
    "    FP += ((pred_choice == 1) & (target == 0)).sum()\n",
    "    p = TP / (TP + FP+0.001)\n",
    "    r = TP / (TP + FN+0.001)\n",
    "    F1 = 2 * r * p / (r + p+0.0001)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return acc,p,r,F1\n",
    "\n",
    "# def get_predict_label(logits,threshold):\n",
    "#     for i in range(len(logits)):\n",
    "#         for j in range(len(logits[0])):\n",
    "#             if logits[i][j]>threshold: logits[i][j] = 1\n",
    "#             else: logits[i][j] = 0\n",
    "#     return logits\n",
    "\n",
    "\n",
    "def train_fn(train_dataloader,optimizer,epoch):\n",
    "\n",
    "    print_diff = 50\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0  \n",
    "\n",
    "    with tqdm(train_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "        \n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            label = label.to(device)\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            logits = logits.cpu()\n",
    "            # t.set_postfix(loss=loss.item(),min=torch.min(logits),max=torch.max(logits),mean = torch.mean(logits))\n",
    "            t.set_postfix(loss=loss.item())\n",
    "        print('Train Loss:',running_loss/len(train_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_fn(valid_dataloader,epoch):\n",
    "    predict=np.zeros((len(valid_index_list),class_num))\n",
    "\n",
    "    model.eval()\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    total_acc = 0     \n",
    "\n",
    "    case_precision = 0\n",
    "    case_recall = 0\n",
    "    case_f1 = 0\n",
    "    case_acc = 0  \n",
    "    running_loss =0\n",
    "    n=0\n",
    "    with tqdm(valid_dataloader) as t:\n",
    "        for i, data in enumerate(t):\n",
    "            id,fact, c_label= data\n",
    "\n",
    "            # tokenize the data text\n",
    "            inputs = tokenizer(list(fact), max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "            # move data to device\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            c_label = c_label.to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # forward and backward propagations\n",
    "            loss, logits = model(input_ids, attention_mask, token_type_ids, c_label)\n",
    "            n+=len(id)\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "\n",
    "            threshold = 0\n",
    "            logits[logits>threshold] = 1\n",
    "            logits[logits<=threshold] = 0\n",
    "\n",
    "\n",
    "\n",
    "            logits=logits.cpu().numpy()\n",
    "            # for i in range(logits.shape[0]):\n",
    "            #     c_predict=np.zeros(class_num)\n",
    "            #     c_predict[np.argmax(logits[i])]=1\n",
    "            #     logits[i]=c_predict\n",
    "            \n",
    "            # 单句标签f1\n",
    "            c_label = c_label.cpu().numpy()\n",
    "            for i in range(len(id)):\n",
    "                idx = int(id[i])\n",
    "                row_idx = valid_index_dict[idx]\n",
    "                predict[row_idx] += logits[i]\n",
    "            \n",
    "\n",
    "\n",
    "            for i in range(len(logits)):\n",
    "                acc,p,r,F1 = cal_metrics(logits[i],c_label[i])        \n",
    "                # print(logits[0],label[1])       \n",
    "                total_precision+= p\n",
    "                total_recall+= r\n",
    "                total_f1+= F1\n",
    "                total_acc+= acc\n",
    "\n",
    "        # 案件标签f1\n",
    "        predict[predict>1] = 1\n",
    "        for i in range(predict.shape[0]):\n",
    "\n",
    "            acc,p,r,F1 = cal_metrics(predict[i],label[i]) \n",
    "            case_precision+= p\n",
    "            case_recall+= r\n",
    "            case_f1+= F1\n",
    "            case_acc+= acc\n",
    "        print('Valid Loss:',running_loss/len(valid_dataloader))\n",
    "            \n",
    "        print('Epoch %d Sen Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,total_acc/valid_size,total_precision/valid_size,total_recall/valid_size,total_f1/valid_size,))\n",
    "        print('Epoch %d Case Valid   acc: %.4f, pre: %.4f, rec: %.4f, f1: %.4f' % (epoch+1,case_acc/valid_case_size,case_precision/valid_case_size,case_recall/valid_case_size,case_f1/valid_case_size))\n",
    "  \n",
    "for epoch in range(epochs):\n",
    "    train_fn(train_dataloader,optimizer,epoch)\n",
    "    valid_fn(valid_dataloader,epoch)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "torch.save({'model': model.state_dict()}, 'model_name.pth')\n",
    "\n",
    "## 读取模型\n",
    "model = CaseClassification(class_num=class_num,model_path=model_path).to(device)\n",
    "state_dict = torch.load('model_name.pth')\n",
    "model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils import get_fact, get_level3labels\n",
    "input_path = \"../rawdata/train.json\"\n",
    "output_path = \"./output/result_train.txt\"\n",
    "level3labels = get_level3labels(json.load(open('../rawdata/code_tree.json')))\n",
    "\n",
    "records = json.load(open(input_path, encoding='utf-8'))\n",
    "records = records[0:2]\n",
    "results = []\n",
    "for record in tqdm(records):\n",
    "    fact_list = record['content']\n",
    "    result = set()\n",
    "    for fact in fact_list:\n",
    "        inputs = tokenizer(fact, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # move data to device\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        _, logits = model(input_ids, attention_mask, token_type_ids, torch.zeros(234).to(device))\n",
    "\n",
    "\n",
    "        threshold = 0.5\n",
    "        logits = logits[0]\n",
    "        logits[logits>threshold] = 1\n",
    "        logits[logits<=threshold] = 0\n",
    "\n",
    "        \n",
    "        for p in range(len(logits)):\n",
    "            if logits[p]==1:\n",
    "                result.add(level3labels[p])\n",
    "    result=list(result)\n",
    "    results.append(result)\n",
    "json.dump(results, open(output_path, \"w\", encoding=\"utf8\"), indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
